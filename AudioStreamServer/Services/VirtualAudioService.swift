//
//  VirtualAudioService.swift (ä¿®å¤ç‰ˆæœ¬)
//

import Foundation
import AVFoundation
import Combine

class AudioCaptureService: NSObject, ObservableObject {
    // MARK: - å‘å¸ƒå±æ€§
    @Published var audioLevel: Double = 0.0
    @Published var isCapturing: Bool = false
    @Published var errorMessage: String?
    @Published var audioFormat: String = "æœªé…ç½®"
    @Published var bufferStatus: String = "ç©ºé—²"
    
    // MARK: - éŸ³é¢‘é…ç½®
    private let bufferSize: AVAudioFrameCount = 256
    private var audioFormatDesc: AVAudioFormat?
    
    // MARK: - éŸ³é¢‘å¼•æ“ç»„ä»¶
    private var audioEngine: AVAudioEngine?
    
    // MARK: - éŸ³é¢‘æ•°æ®å¤„ç†
    private var audioBufferSubject = PassthroughSubject<Data, Never>()
    var audioDataPublisher: AnyPublisher<Data, Never> {
        audioBufferSubject.eraseToAnyPublisher()
    }
    
    // MARK: - ç»Ÿè®¡ä¿¡æ¯
    private var totalFramesCaptured: Int64 = 0
    private var lastProcessTime: Date?
    private var bufferQueue: [Data] = []
    private let maxBufferQueueSize = 10
    
    // MARK: - éŸ³é¢‘æ ¼å¼å±æ€§
    var sampleRate: Double {
        return audioFormatDesc?.sampleRate ?? 48000.0
    }
    
    var channelCount: UInt32 {
        return audioFormatDesc?.channelCount ?? 2
    }
    
    // MARK: - ç”Ÿå‘½å‘¨æœŸ
    override init() {
        super.init()
        setupAudioSession()
    }
    
    deinit {
        stopCapture()
    }
    
    // MARK: - å…¬å…±æ–¹æ³•
    func startCapture() throws {
            guard !isCapturing else { return }
            
            do {
                print("ğŸ§ å¼€å§‹éŸ³é¢‘æ•è·...")
                
                // é‡ç½®çŠ¶æ€
                resetCaptureState()
                
                // åˆ›å»ºå¹¶é…ç½®éŸ³é¢‘å¼•æ“
                try setupAudioEngine()
                
                // å¯åŠ¨éŸ³é¢‘å¼•æ“
                try audioEngine?.start()
                
                // åœ¨ä¸»çº¿ç¨‹æ›´æ–°çŠ¶æ€
                DispatchQueue.main.async {
                    self.isCapturing = true
                    self.errorMessage = nil
                    self.bufferStatus = "è¿è¡Œä¸­"
                }
                
                print("âœ… éŸ³é¢‘æ•è·å·²å¯åŠ¨ - æ ¼å¼: \(self.audioFormat)")
                
            } catch {
                let errorMsg = "âŒ å¯åŠ¨éŸ³é¢‘æ•è·å¤±è´¥: \(error.localizedDescription)"
                
                // åœ¨ä¸»çº¿ç¨‹æ›´æ–°é”™è¯¯ä¿¡æ¯
                DispatchQueue.main.async {
                    self.errorMessage = errorMsg
                    self.bufferStatus = "é”™è¯¯"
                }
                
                print(errorMsg)
                throw error
            }
        }
        
        func stopCapture() {
            guard isCapturing else { return }
            
            print("ğŸ›‘ åœæ­¢éŸ³é¢‘æ•è·...")
            
            audioEngine?.stop()
            audioEngine?.inputNode.removeTap(onBus: 0)
            audioEngine = nil
            
            // åœ¨ä¸»çº¿ç¨‹æ›´æ–°çŠ¶æ€
            DispatchQueue.main.async {
                self.isCapturing = false
                self.audioLevel = 0.0
                self.bufferStatus = "å·²åœæ­¢"
            }
            
            bufferQueue.removeAll()
            
            print("âœ… éŸ³é¢‘æ•è·å·²åœæ­¢")
    }
    
    // MARK: - éŸ³é¢‘ä¼šè¯é…ç½®
    private func setupAudioSession() {
        print("âœ… macOS éŸ³é¢‘ä¼šè¯é…ç½®å®Œæˆ")
    }
    
    // MARK: - éŸ³é¢‘å¼•æ“é…ç½®
    private func setupAudioEngine() throws {
        audioEngine = AVAudioEngine()
        
        guard let audioEngine = audioEngine else {
            throw NSError(domain: "AudioCaptureService", code: 1,
                          userInfo: [NSLocalizedDescriptionKey: "æ— æ³•åˆ›å»ºéŸ³é¢‘å¼•æ“"])
        }
        
        let inputNode = audioEngine.inputNode
        
        // è·å–è¾“å…¥æ ¼å¼
        let inputFormat = inputNode.outputFormat(forBus: 0)
        guard inputFormat.sampleRate > 0 else {
            throw NSError(domain: "AudioCaptureService", code: 2,
                          userInfo: [NSLocalizedDescriptionKey: "è¾“å…¥èŠ‚ç‚¹æ ¼å¼æ— æ•ˆ"])
        }
        
        audioFormatDesc = inputFormat
        updateAudioFormatDisplay()
        
        print("ğŸ›ï¸ éŸ³é¢‘è¾“å…¥æ ¼å¼: \(inputFormat.description)")
        
        // å®‰è£…éŸ³é¢‘æ•è·tap
        installAudioTap(inputNode: inputNode, format: inputFormat)
        
        // è¿æ¥èŠ‚ç‚¹ï¼ˆè™½ç„¶æˆ‘ä»¬ä¸éœ€è¦è¾“å‡ºï¼Œä½†ä¿æŒå¼•æ“è¿è¡Œï¼‰
        audioEngine.connect(inputNode, to: audioEngine.mainMixerNode, format: inputFormat)
        
        // å‡†å¤‡éŸ³é¢‘å¼•æ“
        audioEngine.prepare()
    }
    
    private func installAudioTap(inputNode: AVAudioInputNode, format: AVAudioFormat) {
        inputNode.installTap(onBus: 0, bufferSize: bufferSize, format: format) { [weak self] (buffer, time) in
            self?.processAudioBuffer(buffer, time: time)
        }
    }
    
    // åœ¨ processAudioBuffer ä¸­ç«‹å³å‘é€æ•°æ®ï¼Œä¸ç­‰å¾…ç¼“å†²åŒºç§¯ç´¯
    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer, time: AVAudioTime) {
        guard isCapturing else { return }
        
        // ç«‹å³æ›´æ–°å¤„ç†æ—¶é—´
        lastProcessTime = Date()
        
        // è®¡ç®—éŸ³é¢‘ç”µå¹³
        calculateAudioLevel(from: buffer)
        
        // ç«‹å³è½¬æ¢å¹¶å‘é€éŸ³é¢‘æ•°æ®ï¼ˆä¸ç­‰å¾…ç¼“å†²åŒºç§¯ç´¯ï¼‰
        if let audioData = convertToAudioData(buffer: buffer) {
            totalFramesCaptured += Int64(buffer.frameLength)
            audioBufferSubject.send(audioData)
        }
        
        // ç§»é™¤ç¼“å†²åŒºé˜Ÿåˆ—ç®¡ç†ï¼Œç›´æ¥å‘é€
    }
    
    private func calculateAudioLevel(from buffer: AVAudioPCMBuffer) {
            guard let channelData = buffer.floatChannelData?[0] else { return }
            
            let frameLength = Int(buffer.frameLength)
            var sum: Float = 0.0
            
            for i in 0..<frameLength {
                let sample = channelData[i]
                sum += sample * sample
            }
            
            let rms = sqrt(sum / Float(frameLength))
            let db = 20.0 * log10(rms + 1e-10)
            let normalizedLevel = max(0.0, min(1.0, (db + 60.0) / 60.0))
            
            // åœ¨ä¸»çº¿ç¨‹æ›´æ–°éŸ³é¢‘ç”µå¹³
            DispatchQueue.main.async {
                self.audioLevel = Double(normalizedLevel)
            }
    }
    
    private func convertToAudioData(buffer: AVAudioPCMBuffer) -> Data? {
        guard let channelData = buffer.floatChannelData?[0] else { return nil }
        
        let frameLength = Int(buffer.frameLength)
        let dataSize = frameLength * MemoryLayout<Float>.size
        
        return channelData.withMemoryRebound(to: UInt8.self, capacity: dataSize) { pointer in
            return Data(bytes: pointer, count: dataSize)
        }
    }
    
    private func manageBufferQueue(_ audioData: Data) {
        bufferQueue.append(audioData)
        
        // é™åˆ¶é˜Ÿåˆ—å¤§å°
        if bufferQueue.count > maxBufferQueueSize {
            bufferQueue.removeFirst()
        }
        
        // æ›´æ–°ç¼“å†²åŒºçŠ¶æ€
        updateBufferStatus()
    }
    
    private func updateBufferStatus() {
            let status: String
            if bufferQueue.count == 0 {
                status = "ç©ºé—²"
            } else if bufferQueue.count < maxBufferQueueSize / 2 {
                status = "æ­£å¸¸"
            } else {
                status = "ç¹å¿™(\(bufferQueue.count)/\(maxBufferQueueSize))"
            }
            
            // åœ¨ä¸»çº¿ç¨‹æ›´æ–°ç¼“å†²åŒºçŠ¶æ€
            DispatchQueue.main.async {
                self.bufferStatus = status
            }
    }
    
    private func updateAudioFormatDisplay() {
        guard let format = audioFormatDesc else {
            audioFormat = "æœªé…ç½®"
            return
        }
        
        audioFormat = "\(Int(format.sampleRate))Hz â€¢ \(format.channelCount)å£°é“ â€¢ 32-bit Float"
    }
    
    private func resetCaptureState() {
        totalFramesCaptured = 0
        lastProcessTime = nil
        bufferQueue.removeAll()
        audioLevel = 0.0
    }
}
